"""
A Python script for multi-agent collaborative storytelling using the autogen library 
and OpenTelemetry for tracing interactions. The script configures agents with distinct roles, 
sets up a group chat manager, and uses Promptflow tracing for workflow monitoring.

Features:
- Multi-agent collaboration using GPT-based models.
- Structured interaction tracing for debugging and analysis.
- Dynamic model configuration from JSON files.
"""

import os
import json
import logging
from dotenv import load_dotenv
from autogen import UserProxyAgent, ConversableAgent, GroupChat, GroupChatManager, config_list_from_json
from autogen.retrieve_utils import TEXT_FORMATS
from promptflow.tracing import start_trace
from opentelemetry import trace
from opentelemetry.trace.status import Status, StatusCode

# Load environment variables
load_dotenv()
os.environ["OPENAI_API_KEY"] = "Your API Key"

# Set up logging
logger = logging.getLogger()
logging.basicConfig(level=logging.INFO)

# Configuration for model loading
env_or_file = "OAI_CONFIG_LIST.json"

config_list = config_list_from_json(
    env_or_file,
    filter_dict={
        "model": {
            "gpt-35-turbo",
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k",
            "gpt-3.5-turbo-0301",
            "chatgpt-35-turbo-0301",
            "gpt-35-turbo-v0301",
            "gpt-4o-mini"
        },
    },
)

llm_config = {"config_list": config_list, "cache_seed": 42}

# Log available models
if config_list:
    logger.info(f"Models to use: {[config['model'] for config in config_list]}")
else:
    logger.error("No models found in configuration list.")

# Define agents
user_proxy = UserProxyAgent(
    name="Chinmay",
    system_message="A human admin.",
    code_execution_config={
        "last_n_messages": 2,
        "work_dir": "groupchat",
        "use_docker": False,
    },
    human_input_mode="TERMINATE"
)

assistant = ConversableAgent(
    name="Sameer",
    system_message="Create a story on a given topic.",
    llm_config=llm_config,
)

helper = ConversableAgent(
    name="Adam",
    system_message="Check for tone, jump scares, and other factors to make the story interesting.",
    llm_config=llm_config,
)

helper2 = ConversableAgent(
    name="Sam",
    system_message="Review the story and give suggestions to make it stunning.",
    llm_config=llm_config,
)

# Set up GroupChat and Manager
groupchat = GroupChat(
    agents=[assistant, helper, helper2],
    messages=[],
    max_round=12,
    speaker_selection_method="round_robin"
)

manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)

# Start tracing with Promptflow
start_trace(collection="autogen-groupchat")
tracer = trace.get_tracer("Story_teller")

# Main interaction tracing
with tracer.start_as_current_span("autogen-main") as main_span:
    try:
        # User's initial message
        message = "Write a story about stars and explain it"
        user_proxy.initiate_chat(manager, message=message)

        # Trace user input
        main_span.set_attribute("custom", "custom attribute value")
        main_span.add_event("promptflow.function.inputs", {
            "payload": json.dumps({"message": message})
        })

        # Trace agent interactions
        for idx, interaction in enumerate(groupchat.messages):
            with tracer.start_as_current_span(f"interaction-{idx}") as span:
                try:
                    # Extract sender and message details
                    sender = interaction.get("sender", {}).get("name", "Unknown Sender") if isinstance(interaction, dict) else getattr(interaction.sender, "name", "Unknown Sender")
                    message_content = interaction.get("content", "No content") if isinstance(interaction, dict) else getattr(interaction, "content", "No content")

                    # Add interaction details to the span
                    span.set_attribute("agent", sender)
                    span.set_attribute("message", message_content)
                    span.add_event("agent.message_received", {
                        "sender": sender,
                        "message": message_content,
                    })
                    span.set_status(Status(StatusCode.OK))
                except Exception as e:
                    logger.error(f"Error in interaction-{idx}: {e}")
                    span.set_status(Status(StatusCode.ERROR, description=str(e)))
                    span.add_event("error", {"exception": str(e)})

        # Log final output
        main_span.set_status(Status(StatusCode.OK))
        main_span.add_event("promptflow.function.output", {
            "payload": json.dumps(user_proxy.last_message())
        })
    except Exception as e:
        logger.error(f"Error in main interaction: {e}")
        main_span.set_status(Status(StatusCode.ERROR, description=str(e)))
        main_span.add_event("error", {"exception": str(e)})
